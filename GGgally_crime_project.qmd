---
title: "Indiacrime_project"
---

# Install Packages

```{r}
pacman::p_load(sf, tmap, kableExtra, tidyverse, sfdep, readxl, plyr, Kendall, plotly, dplyr,stringdist, spdep)
```

# Aspatial Data Pre-Processing

## Read Data File (IPC)

```{r}
#| eval: false
IPC <- read_xlsx("data/aspatial/1 Districtwise IPC Crimes_2021.xlsx")
```

We remove the first 3 rows as they are just headings and have NA values. We will not be using them anyways.

```{r}
#| eval: false
IPC <- IPC %>%
  slice(5:n())
```

### Convert to numeric

```{r}
#| eval: false
df <- IPC %>%
  select(-c("...1", "...2")) %>%
  mutate_if(is.character,as.numeric)
```

### Group data to reduce cardinality

```{r}
#| eval: false
temp_df <- df |> rowwise() |>
  summarise(murders = sum(`Murder (Sec.302 IPC)`,`Dowry Deaths (Sec.304-B IPC)`),
            attempted_muders = sum(`Culpable Homicide not amounting to Murder (Sec.304 IPC)`,`Abetment of Suicide (Sec.305/306 IPC)`, `Attempt to Commit Murder (Sec.307 IPC)`, `Attempt to commit Culpable Homicide (Sec.308 IPC)`, `Attempt to Commit Suicide (Sec.309 IPC)`, `Miscarriage, Infanticide, Foeticide and Abandonment (Sec.313 to 318 IPC) \r\n`),
            death_negligence = sum(`Causing Death by Negligence`,`...6`,`...7`,`...8`,`...9`,`...10`,`...11`,`...12`),
            hurt = sum(`Hurt`,`Wrongful Restraint/Confinement (Sec.341 to 348 IPC)`),
            women_assault = `Assault on Women with Intent to Outrage her Modesty`,
            kidnapping_abduction = `Kidnapping and Abduction`,
            human_exploitation = sum(`Human Trafficking (U/S 370)`, `Exploitation of Trafficked Person 370A IPC)`, `Selling of Minors for Prostitution (Sec.372 IPC)`, `Buying of Minors for Prostitution (Sec.373 IPC)`, `Rape (Sec.376 IPC)`, `Attempt to Commit Rape (Sec.376/511 IPC)`, `Unnatural Offences (Sec.377 IPC)`),
            state_offence = `...64`,
            public_tranquility = `Offences against Public Tranquillity (Total)`,
            movable_property = `Offences against Property (Total)`,
            document_fraud = `Offences Relating to Documents & Property Marks (Total)`,
            miscellaneous = sum(`Miscellaneous IPC Crimes(Total)`, `...143`))
```

### Join into df

```{r}
#| eval: false
df <- cbind(IPC$"...1", IPC$"...2", temp_df) %>%
       rename("State" = "IPC$...1",
              "District" = "IPC$...2")
```

### Save as rds

```{r}
#| eval: false
write_rds(df, "data/rds/IPC.rds")
```

### Read IPC rds file

```{r}
IPC_new <- read_rds("data/rds/IPC.rds")
```

## Read Data File (SLL)

```{r}
#| eval: false
SLL <- read_xlsx("data/aspatial/2 Districtwise SLL Crimes_2021.xlsx") %>%
  slice(5:n())
```

```{r}
#| eval: false
df <- SLL %>%
  select(-c("...1", "...2")) %>%
  mutate_if(is.character,as.numeric)
```

### Group data to reduce cardinality

```{r}
#| eval: false
temp_df <- df |> rowwise() |>
  summarise(women = `...7`,
            children = `...14`,
            st_sc = `...19`,
            state = `...23`,
            arms_related = `...31`,
            information_infringement = `...35`,
            trade_finance = `...42`,
            narcotics = `...48`,
            environment = `...56`,
            foreigner = `...61`,
            railway = `...64`,
            media = `...69`,
            food = `...73`,
            others = sum(`...91`, `Other SLL Crimes`))
```

### Join into df

```{r}
#| eval: false
df <- cbind(SLL$"...1", SLL$"...2", temp_df) %>%
       rename("State" = "SLL$...1",
              "District" = "SLL$...2")
```

### Save as rds

```{r}
#| eval: false
write_rds(df, "data/rds/SLL.rds")
```

### Read SLL rds file

```{r}
SLL_new <- read_rds("data/rds/SLL.rds")
```

# Geospatial Data Pre-Processing

## Read Data File (India District level)

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                    layer = "output")
mpsz <- subset(mpsz, select = c(statename, distname)) 
```

## IPC Dataset

```{r}
#| eval: false
aspatial_states <-  unique(IPC$State)
geospatial_states <- unique(mpsz$statename)
common_states <- intersect(aspatial_states, geospatial_states)


aspatial_minus_geospatial_states <- setdiff(aspatial_states, common_states)
geospatial_minus_aspatial_states <- setdiff(geospatial_states, common_states)
```

```{r}
#| eval: false
aspatial_minus_geospatial_states
```

```{r}
#| eval: false
geospatial_minus_aspatial_states
```

Here, the Aspatial naming will be followed

```{r}
mpsz <- mpsz %>%
        mutate(statename = recode(statename,
                                  "Chhatisgarh" = "Chhattisgarh",
                                  "Orissa" = "Odisha",
                                  "Andaman & Nicobar Islands" = "A&N Islands",
                                  "Daman & Diu" = "D&N Haveli and Daman & Diu",
                                  "Dadra & Nagar Haveli" = "D&N Haveli and Daman & Diu",
                                  "NCT of Delhi" = "Delhi",
                                  "Pondicherry" = "Puducherry"))
```

District Version

```{r}
#| eval: false
aspatial_districts <-  unique(IPC$District)
geospatial_districts <- unique(mpsz$distname)
common_districts <- intersect(aspatial_districts, geospatial_districts)


aspatial_minus_geospatial_districts <- setdiff(aspatial_districts, common_districts)
geospatial_minus_aspatial_districts <- setdiff(geospatial_districts, common_districts)
```

```{r}
#| eval: false
aspatial_minus_geospatial_districts
```

```{r}
#| eval: false
geospatial_minus_aspatial_districts
```

Too many here!

Here, the geospatial naming will be followed!

```{r}
#| eval: false
distance_df <- data.frame(matrix(nrow = 0, ncol = 3))
colnames(distance_df) = c("aspatial", "similarity", "closest geospatial")
```

```{r}
#| eval: false
new_aspatial <- list()
threshold <- 0.149 # more similar means smaller value
#aspatial_districts <- IPC$District
num <- nrow(IPC)

for(i in 1:num) {
  
  row <- IPC[i,]
  state <- row$State
  value <- row$District
  
  distances <- stringdist(value, geospatial_districts, method = "jw") #distance matrix
  closest_match_index <- which.min(distances) #index
  closest_match <- geospatial_districts[closest_match_index]
  closest_match_state <- mpsz[mpsz$distname == closest_match, ]$statename
  
  #similar enough and same state
  if ((distances[closest_match_index] <= threshold) && (state == closest_match_state)) {
    new_aspatial <- c(new_aspatial, closest_match)
    
  } 
  #not similar enough 
  else {
    new_aspatial <- c(new_aspatial, value)
  }
  
  #distance_df[nrow(distance_df) + 1, ] = c(value, distances[closest_match_index], closest_match)
  
}
new_aspatial <- flatten_chr(new_aspatial)
IPC$District <- new_aspatial
```

Numbers after fuzzy search!

```{r}
#| eval: false
aspatial_districts <-  unique(IPC$District)
geospatial_districts <- unique(mpsz$distname)
common_districts <- intersect(aspatial_districts, geospatial_districts)


aspatial_minus_geospatial_districts <- setdiff(aspatial_districts, common_districts)
geospatial_minus_aspatial_districts <- setdiff(geospatial_districts, common_districts)
```

```{r}
#| eval: false
aspatial_minus_geospatial_districts
```

```{r}
#| eval: false
geospatial_minus_aspatial_districts
```

Much better but still a lot of work left to preprocess! Hence, we shall move to CSV for ease of preprocessing.

```{r}
#| eval: false
#write.csv(data.frame(aspatial_minus_geospatial_districts), "asp.csv", row.names = FALSE)
#write.csv(mpsz %>% st_drop_geometry(), "geomap.csv", row.names = FALSE)
```

------------------------------------------------------------------------

After checking, here's the new list.

There were a lot of mismatch in the data between the geospatial and aspatial data as the geospatial data is from 2014 and the aspatial data is from 2021. Furthermore, the aspatial data is very unclean as it contains sub-districts and towns along with the district names.

```{r}
#| eval: false
#IPC <- read_rds("data/aspatial/rds/IPC.rds")
unique_aspatial_districts <- read.csv(file = 'asp.csv')

#replace the names in the IPC file
IPC$District[match(unique_aspatial_districts$oldname, IPC$District)] <- unique_aspatial_districts$newname

#remove irrelevant variables
IPC <- IPC[IPC$District != "remove", ]
```

When we check again, we see that the mismatch values have decreased.

```{r}
#| eval: false
aspatial_districts <-  unique(IPC$District)
geospatial_districts <- unique(mpsz$distname)
common_districts <- intersect(aspatial_districts, geospatial_districts)


aspatial_minus_geospatial_districts <- setdiff(aspatial_districts, common_districts)
geospatial_minus_aspatial_districts <- setdiff(geospatial_districts, common_districts)
```

Other than Warangal and Kashmir that have been changed, Mumbai Subarbans need to be added into Mumbai and the remaining data is to be removed.

```{r}
#| eval: false
aspatial_minus_geospatial_districts
```

```{r}
#| eval: false
geospatial_minus_aspatial_districts
```

For the aspatial data, CID, GRP, Cyber, Cyber Branch and Cyber Cell are not needed.

```{r}
#| eval: false
IPC <-IPC[!(IPC$District=="CID" | IPC$District=="GRP" | IPC$District=="Crime Branch" | IPC$District=="Cyber" | IPC$District=="Cyber Cell"),]
```

For the geospatial data, renaming has to be done to accomodate Kashmir, Warangal (R) and (U) and Mumbai and Mumbai Suburbans have to be joined. Further changes will be checked after.

```{r}
#| eval: false
#change names
mpsz$distname[mpsz$distname == "DATA NOT AVAILABLE"] <- "Kashmir"
mpsz$distname[mpsz$distname == "Warangal (R)"] <- "Warangal"
mpsz$distname[mpsz$distname == "Warangal (U)"] <- "Warangal"
mpsz$distname[mpsz$distname == "Mumbai Suburban"] <- "Mumbai"

#combine geometry
mpsz$geometry[mpsz$distname == "Warangal"] <- st_combine(mpsz$geometry[mpsz$distname == "Warangal"])
mpsz$geometry[mpsz$distname == "Mumbai"] <- st_combine(mpsz$geometry[mpsz$distname == "Mumbai"])

```

```{r}
#| eval: false
#check combination 
#plot(test_mpsz[445,])
```

Now there won't be any aspatial values left to process. The geospatial value will require manual lookup and changes will be made to the aspatial dataset.

Most of the Geospatial values are either incomplete district names for certain states or retired restricts.

```{r}
#| eval: false
geospatial_minus_aspatial_districts
```

```{r}
#| eval: false
#Sikkim State
IPC$District[IPC$District == "North" & IPC$State == "Sikkim"] <- "North District"
IPC$District[IPC$District == "East" & IPC$State == "Sikkim"] <- "East District"
IPC$District[IPC$District == "South" & IPC$State == "Sikkim"] <- "South District"
IPC$District[IPC$District == "West" & IPC$State == "Sikkim"] <- "West District"

#Tripura State
IPC$District[IPC$District == "West" & IPC$State == "Tripura"] <- "West Tripura"
IPC$District[IPC$District == "North" & IPC$State == "Tripura"] <- "North Tripura"
IPC$District[IPC$District == "South" & IPC$State == "Tripura"] <- "South Tripura"

#Pondicherry State - Mahe and Yanam - suss - merge geometry 
mpsz$distname[mpsz$distname == "Mahe"] <- "Puducherry"
mpsz$distname[mpsz$distname == "Yanam"] <- "Puducherry"


#Odisha State - no data so merge into Puri
mpsz$distname[mpsz$distname == "Khordha"] <- "Puri"
mpsz$geometry[mpsz$distname == "Puri"] <- st_combine(mpsz$geometry[mpsz$distname == "Puri"])


#Telangana State
mpsz$distname[mpsz$distname == "Jangaon"] <- "Warangal"
mpsz$geometry[mpsz$distname == "Warangal"] <- st_combine(mpsz$geometry[mpsz$distname == "Warangal"])
mpsz$distname[mpsz$distname == "Mancherial"] <- "Adilabad"
mpsz$geometry[mpsz$distname == "Adilabad"] <- st_combine(mpsz$geometry[mpsz$distname == "Adilabad"])
mpsz$distname[mpsz$distname == "Rangareddy"] <- "Hydrabad"
mpsz$geometry[mpsz$distname == "Hydrabad"] <- st_combine(mpsz$geometry[mpsz$distname == "Hydrabad"])
mpsz$distname[mpsz$distname == "Peddapalli"] <- "Karimnagar"
mpsz$geometry[mpsz$distname == "Karimnagar"] <- st_combine(mpsz$geometry[mpsz$distname == "Karimnagar"])
mpsz$distname[mpsz$distname == "Yadadri"] <- "Nalgonda"
mpsz$geometry[mpsz$distname == "Nalgonda"] <- st_combine(mpsz$geometry[mpsz$distname == "Nalgonda"])
mpsz <- mpsz[mpsz$distname != "Medchal", ] #remove
```

Now lets remove the duplicates in geospatial data

```{r}
#| eval: false
#remove duplicate rows
mpsz <- mpsz[-c(677, 662, 448, 669, 672, 679, 687),]

#reset index
rownames(mpsz) <- NULL
```

Lets group some values in aspatial data

```{r}
#| eval: false
IPC <- IPC %>% group_by(State,District) %>% 
  summarise(across(c(colnames(IPC)[3:14]),sum),
            .groups = 'drop') %>%
  as.data.frame()
```

Now, lets write into RDS

```{r}
#| eval: false
write_rds(IPC, "data/aspatial/rds/IPC_clean.rds")
write_rds(mpsz, "data/geospatial/rds/map_clean.rds")
```

------------------------------------------------------------------------

## SSL Dataset

Now, lets start the pre-processing for SSL dataset

```{r}
#| eval: false
SLL <- read_rds("data/aspatial/rds/SLL.rds")
mpsz <- read_rds("data/geospatial/rds/map_clean.rds")
```

State names have already been cleaned, so only district names have to be cleaned.

```{r}
#| eval: false
aspatial_districts <-  unique(SLL$District)
geospatial_districts <- unique(mpsz$distname)
common_districts <- intersect(aspatial_districts, geospatial_districts)


aspatial_minus_geospatial_districts <- setdiff(aspatial_districts, common_districts)
geospatial_minus_aspatial_districts <- setdiff(geospatial_districts, common_districts)
```

```{r}
#| eval: false
aspatial_minus_geospatial_districts
```

```{r}
#| eval: false
geospatial_minus_aspatial_districts
```

Too many mismatched values as expected, hence, we shall perform Fuzzy Search again.

```{r}
#| eval: false
new_aspatial <- list()
threshold <- 0.149 # more similar means smaller value
#aspatial_districts <- SLL$District
num <- nrow(SLL)

for(i in 1:num) {
  
  row <- SLL[i,]
  state <- row$State
  value <- row$District
  
  distances <- stringdist(value, geospatial_districts, method = "jw") #distance matrix
  closest_match_index <- which.min(distances) #index
  closest_match <- geospatial_districts[closest_match_index]
  closest_match_state <- mpsz[mpsz$distname == closest_match, ]$statename
  
  #similar enough and same state
  if ((distances[closest_match_index] <= threshold) && (state == closest_match_state)) {
    new_aspatial <- c(new_aspatial, closest_match)
    
  } 
  #not similar enough 
  else {
    new_aspatial <- c(new_aspatial, value)
  }
  
  #distance_df[nrow(distance_df) + 1, ] = c(value, distances[closest_match_index], closest_match)
  
}
new_aspatial <- flatten_chr(new_aspatial)
SLL$District <- new_aspatial
```

Check for Mismatch

```{r}
#| eval: false
aspatial_districts <-  unique(SLL$District)
geospatial_districts <- unique(mpsz$distname)
common_districts <- intersect(aspatial_districts, geospatial_districts)


aspatial_minus_geospatial_districts <- setdiff(aspatial_districts, common_districts)
geospatial_minus_aspatial_districts <- setdiff(geospatial_districts, common_districts)
```

```{r}
#| eval: false
aspatial_minus_geospatial_districts
```

```{r}
#| eval: false
geospatial_minus_aspatial_districts
```

Mismatched values have come down from 415 to 307 for the aspatial values and from 209 to 119 for geospatial values. However, there is a lot of preprocessing left to be done manually.

```{r}
#| eval: false
#write.csv(data.frame(aspatial_minus_geospatial_districts), "sll_asp.csv", row.names = FALSE)
#write.csv(mpsz %>% st_drop_geometry(), "geomap.csv", row.names = FALSE)
```

As the IPC and SSL data follow the same format, the values after preprocessing can be reused.

```{r}
#| eval: false
#SLL <- read_rds("data/aspatial/rds/SLL.rds")
unique_aspatial_districts <- read.csv(file = 'sll_asp.csv')

#replace the names in the IPC file
SLL$District[match(unique_aspatial_districts$oldname, SLL$District)] <- unique_aspatial_districts$newname

#remove irrelevant variables
SLL <- SLL[SLL$District != "remove", ]
```

When we check again, the mismatched values have decreased.

```{r}
#| eval: false
aspatial_districts <-  unique(SLL$District)
geospatial_districts <- unique(mpsz$distname)
common_districts <- intersect(aspatial_districts, geospatial_districts)


aspatial_minus_geospatial_districts <- setdiff(aspatial_districts, common_districts)
geospatial_minus_aspatial_districts <- setdiff(geospatial_districts, common_districts)
```

```{r}
#| eval: false
aspatial_minus_geospatial_districts
geospatial_minus_aspatial_districts
```

For the aspatial data, CID, GRP, Cyber, Crime Branch and Cyber Cell are not needed.

```{r}
#| eval: false
SLL <-SLL[!(SLL$District=="CID" | SLL$District=="GRP" | SLL$District=="Crime Branch" | SLL$District=="Cyber" | SLL$District=="Cyber Cell"),]
```

```{r}
#| eval: false
#Sikkim State
SLL$District[SLL$District == "North" & SLL$State == "Sikkim"] <- "North District"
SLL$District[SLL$District == "East" & SLL$State == "Sikkim"] <- "East District"
SLL$District[SLL$District == "South" & SLL$State == "Sikkim"] <- "South District"
SLL$District[SLL$District == "West" & SLL$State == "Sikkim"] <- "West District"

#Tripura State
SLL$District[SLL$District == "West" & SLL$State == "Tripura"] <- "West Tripura"
SLL$District[SLL$District == "North" & SLL$State == "Tripura"] <- "North Tripura"
SLL$District[SLL$District == "South" & SLL$State == "Tripura"] <- "South Tripura"
```

After this district from Anantaput from Andhra Pradesh will be left because there are no values for it. This can be left as it is or the data can be removed from IPC for the sake of uniformity. We will be leaving it as it is for now.

For aspatial data, we can group values.

```{r}
#| eval: false
SLL <- SLL %>% group_by(State,District) %>% 
  summarise(across(c(colnames(SLL)[3:16]),sum),
            .groups = 'drop') %>%
  as.data.frame()
```

Now, lets write into RDS

```{r}
#| eval: false
write_rds(SLL, "data/aspatial/rds/SLL_clean.rds")
```

# Joining Aspatial and Geospatial Data files

```{r}
IPC_Cleaned <- read_rds("data/rds/IPC_clean.rds")
```

```{r}
SLL_Cleaned <- read_rds("data/rds/SLL_clean.rds")
```

```{r}
map_Cleaned <- read_rds("data/rds/map_clean.rds")
```

Lets check the CRS of our geospatial data -

```{r}
st_crs(map_Cleaned)
```

The CRS is WG84, we will not be changed the CRS as upon searching for the EPSG code for India, we found that it is recommended to use WG84 for each state of India and there is no CRS for the whole of India. And hence, we won't be changing the CRS.

Now, lets see if our geospatial data is valid.

```{r}
count(st_is_valid(map_Cleaned))
```

We can see that the output returns 7 False values. Let's use the function st_make_valid to make the values valid.

```{r}
st_make_valid(map_Cleaned)
```

Lets double check if there are any invalid values.

```{r}
count(st_is_valid(map_Cleaned))
```

As we can see, st_is_valid did not work. Hence, lets take a deeper look into which values are invalid values.

```{r}
# Find indices of invalid geometries
invalid_idx <- which(!st_is_valid(map_Cleaned))

# Subset the data to get only the invalid geometries
invalid_geoms <- map_Cleaned[invalid_idx,]

# View the invalid geometries
invalid_geoms
```

There are 2 values for Puri, Odisha, lets check if they are the same!

```{r}
identical(map_Cleaned[531,"geometry"] , map_Cleaned[537,"geometry"])
```

As, we can we see there is a duplication of data for Puri, Odisha, we can remove one of them.

```{r}
map_Cleaned <- map_Cleaned[-537,]
```

Now, lets compare the geometry values of map_Cleaned with that of the orginal geometry values of mpsz (the geospatial data without any data pre-processing). We are afraid that the geometry values might have changed while data pre-processing.

```{r}
identical(map_Cleaned[420,"geometry"] , mpsz[420,"geometry"])
identical(map_Cleaned[445,"geometry"] , mpsz[445,"geometry"])
identical(map_Cleaned[531,"geometry"] , mpsz[532,"geometry"])
identical(map_Cleaned[664,"geometry"] , mpsz[666,"geometry"])
identical(map_Cleaned[663,"geometry"] , mpsz[665,"geometry"])
identical(map_Cleaned[667,"geometry"] , mpsz[670,"geometry"])
```

```{r}
map_Cleaned[420,"geometry"] = mpsz[420,"geometry"]
map_Cleaned[445,"geometry"] = mpsz[445,"geometry"]
map_Cleaned[531,"geometry"] = mpsz[532,"geometry"]
map_Cleaned[664,"geometry"] = mpsz[666,"geometry"]
map_Cleaned[663,"geometry"] = mpsz[665,"geometry"]
map_Cleaned[667,"geometry"] = mpsz[670,"geometry"]
```

Now, lets check if our above method worked.

```{r}
# Find indices of invalid geometries
invalid_idx <- which(!st_is_valid(map_Cleaned))

# Subset the data to get only the invalid geometries
invalid_geoms <- map_Cleaned[invalid_idx,]

# View the invalid geometries
invalid_geoms
```

As we can see, the districts - Hydrebad and Nalgonda from Telgana are still invalid.

**SO WHAT DO WE DOO????**\

```{r}
plot(st_geometry(map_Cleaned))
```

```{r}
#| eval: false
tm_shape(map_Cleaned) + 
  tm_polygons("distname") +
  tmap_options(check.and.fix = TRUE)
```

```{r}
IPC_combined_India_districtlevel <- left_join(IPC_Cleaned, map_Cleaned,by=c(
                                "District"="distname"))
```

Lets check if any of the Districts are duplicated.

```{r}
which(duplicated(IPC_combined_India_districtlevel$District))
```

-   63, 361 and 362 - Aurangabad in Bihar, but for one of the values, the statename is 'Maharashtra' and vice-verca

-   266 and 267 - Bijapur in Karnata, but for one of the values, the statename is 'Chhattisgarh' and vice-verca

-   Rest all are Duplicated Values

-   155

-   465, 466 - Not sure as it is same place but with different polygon values

-   582

```{r}
IPC_combined_India_districtlevel <- IPC_combined_India_districtlevel[-c(103, 106, 108, 124, 202, 205, 385, 516, 601, 624, 652),]

IPC_combined_India_districtlevel <- subset(IPC_combined_India_districtlevel, !(State == "Bihar" & statename == 'Maharashtra'))
IPC_combined_India_districtlevel <- subset(IPC_combined_India_districtlevel, !(State == "Maharashtra" & statename == 'Bihar'))
IPC_combined_India_districtlevel <- subset(IPC_combined_India_districtlevel, !(State == "Karnataka" & statename == 'Chhattisgarh'))
```

```{r}
SLL_combined_India_districtlevel <- left_join(SLL_Cleaned, map_Cleaned,by=c(
                                "District"="distname"))
```

## **Write into an RDS**

```{r}
write_rds(IPC_combined_India_districtlevel, "data/rds/IPC_combined_India_districtlevel.rds")
```

```{r}
write_rds(SLL_combined_India_districtlevel, "data/rds/SLL_combined_India_districtlevel.rds")
```

```{r}
tmap_options(check.and.fix = TRUE)
```

## Visualise our Joined Data File

```{r}
IPC_combined_India_districtlevel <- st_as_sf(IPC_combined_India_districtlevel)

murders = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("murders") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="Murders Count") + 
  tmap_options(check.and.fix = TRUE)

murders
```

```{r}
attempted_muders = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("attempted_muders") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="attempted_muders Count")

death_negligence = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("death_negligence") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="death_negligence Count")

tmap_arrange(attempted_muders, death_negligence)
```

```{r}
hurt = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("hurt") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="hurt Count")

Women_assault = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("women_assault") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="women_assault Count")

tmap_arrange(hurt, Women_assault)
```

```{r}
kidnapping_abduction = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("kidnapping_abduction") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="kidnapping_abduction Count")

human_exploitation = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("human_exploitation") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="human_exploitation Count")

tmap_arrange(kidnapping_abduction, human_exploitation)
```

```{r}
state_offence = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("state_offence") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="state_offence Count")

public_tranquility = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("public_tranquility") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="public_tranquility Count")

tmap_arrange(state_offence, public_tranquility)
```

```{r}
movable_property = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("movable_property") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="movable_property Count")

document_fraud = tm_shape(IPC_combined_India_districtlevel)+
  tm_fill("document_fraud") +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title="document_fraud Count")

tmap_arrange(movable_property, document_fraud)
```

```{r}
df <- data.frame(
  Crime_Type = c("hurt", "murders", "attempted_muders", "death_negligence", "women_assault", 
             "kidnapping_abduction", "human_exploitation", "state_offence", 
             "public_tranquility", "movable_property", "document_fraud"),
  total = c(sum(IPC_combined_India_districtlevel$hurt), 
            sum(IPC_combined_India_districtlevel$murders),
            sum(IPC_combined_India_districtlevel$attempted_muders),
            sum(IPC_combined_India_districtlevel$death_negligence),
            sum(IPC_combined_India_districtlevel$women_assault),
            sum(IPC_combined_India_districtlevel$kidnapping_abduction),
            sum(IPC_combined_India_districtlevel$human_exploitation),
            sum(IPC_combined_India_districtlevel$state_offence),
            sum(IPC_combined_India_districtlevel$public_tranquility),
            sum(IPC_combined_India_districtlevel$movable_property),
            sum(IPC_combined_India_districtlevel$document_fraud))
)

# Create the bar chart
ggplot(df, aes(x = Crime_Type, y = total, fill=Crime_Type)) +
  geom_bar(stat = "identity") +
  xlab("Crime_Type") +
  ylab("Total") +
  ggtitle("Total by Column") +
  geom_text(aes(label=total), vjust=-0.4)
```

# Global Spatial Autocorrelation

```{r}
IPC_combined_India_districtlevel <- read_rds("data/rds/IPC_combined_India_districtlevel.rds")

SLL_combined_India_districtlevel <- read_rds("data/rds/SLL_combined_India_districtlevel.rds")
```

```{r}
IPC_combined_India_districtlevel <- st_as_sf(IPC_combined_India_districtlevel)
SLL_combined_India_districtlevel <- st_as_sf(SLL_combined_India_districtlevel)
```

```{r}
IPC_combined_India_districtlevel <- st_cast(IPC_combined_India_districtlevel, "MULTIPOLYGON")
SLL_combined_India_districtlevel <- st_cast(SLL_combined_India_districtlevel, "MULTIPOLYGON")
```

```{r}
IPC_combined_India_districtlevel <- st_make_valid(IPC_combined_India_districtlevel)
```

## Computing Contiguity Spatial Weights

```{r}
wm_IPC <- poly2nb(IPC_combined_India_districtlevel, 
                queen=TRUE)
summary(wm_IPC)
```

There are 686 regions in India. There is 1 most connected areas unit with 11 neighbours. And there are 5 regions with just no neighbours.

## Row Standardized Weight Matrix

```{r}
set.ZeroPolicyOption(TRUE)
```

```{r}
rswm_IPC <- nb2listw(wm_IPC, 
                   style="B", 
                   zero.policy = TRUE)
rswm_IPC
```

## Global Spatial Autocorrelation : Moran's L

Moran's L-test

```{r}
moran.test(IPC_combined_India_districtlevel$murders, 
           listw=rswm_IPC, 
           zero.policy = TRUE, 
           na.action=na.omit)
```

Computing Monte Carlo Moran's L

```{r}
set.seed(1234)
bperm= moran.mc(IPC_combined_India_districtlevel$murders, 
                listw=rswm_IPC, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

Visualising Monte Carlo Moran's L

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

## Global Spatial Autocorrelation : Geary's Test

Geary's C Test

```{r}
geary.test(IPC_combined_India_districtlevel$murders, listw=rswm_IPC)
```

Computing Monte Carlo Geary's C

```{r}
set.seed(1234)
bperm_g=geary.mc(IPC_combined_India_districtlevel$murders, 
               listw=rswm_IPC, 
               nsim=999)
bperm_g
```

Visualising Monte Carlo Geary's C

```{r}
hist(bperm_g$res, freq=TRUE, breaks=20, xlab="Simulated Geary c")
abline(v=1, col="red") 
```

# Spatial Correlogram

## Compute and Plot Moran's L Correlogram

```{r}
MI_corr <- sp.correlogram(wm_IPC, 
                          IPC_combined_India_districtlevel$murders, 
                          order=6, 
                          method="I", 
                          style="W",
                          zero.policy = TRUE)
plot(MI_corr)
```

## Compute and Plot Geary's C Correlogram

```{r}
GC_corr <- sp.correlogram(wm_IPC, 
                          IPC_combined_India_districtlevel$murders, 
                          order=6, 
                          method="C", 
                          style="W",
                          zero.policy = TRUE)
plot(GC_corr)
```

# Cluster and Outlier Analysis

## Computing local Moran's L

```{r}
fips <- order(IPC_combined_India_districtlevel$District)
localMI <- localmoran(IPC_combined_India_districtlevel$murders, rswm_IPC)
head(localMI)
```

```{r}
printCoefmat(data.frame(
  localMI[fips,], 
  row.names=IPC_combined_India_districtlevel$District[fips]),
  check.names=FALSE)
```

```{r}
IPC.localMI <- cbind(IPC_combined_India_districtlevel,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

```{r}
localMI.map <- tm_shape(IPC.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5) +
  tmap_options(check.and.fix = TRUE)

pvalue.map <- tm_shape(IPC.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5) + 
  tmap_options(check.and.fix = TRUE)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

# Creating a LISA Map

## Plotting Moran Scatterplot

```{r}
nci_IPC <- moran.plot(IPC_combined_India_districtlevel$murders, rswm_IPC,
                  labels=as.character(IPC_combined_India_districtlevel$District), 
                  xlab="Murders 2021", 
                  ylab="Spatially Lag Murders 2021")
```

## Plotting Moran Scatterplot with standardised variable

```{r}
IPC_combined_India_districtlevel$Z.murders <- scale(IPC_combined_India_districtlevel$murders) %>% 
  as.vector 
```

```{r}
nci2_IPC <- moran.plot(IPC_combined_India_districtlevel$Z.murders, rswm_IPC,
                   labels=as.character(IPC_combined_India_districtlevel$District),
                   xlab="z-murders 2021", 
                   ylab="Spatially Lag z-murders 2021")
```

## Preparing Lisa Map Classes

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
```

```{r}
IPC_combined_India_districtlevel$lag_murders <- lag.listw(rswm_IPC, IPC_combined_India_districtlevel$murders)
DV <- IPC_combined_India_districtlevel$lag_murders - mean(IPC_combined_India_districtlevel$lag_murders)     
```

```{r}
LM_I <- localMI[,1] - mean(localMI[,1])    
```

```{r}
signif <- 0.05       
```

```{r}
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4      
```

```{r}
quadrant[localMI[,5]>signif] <- 0
```

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
IPC_combined_India_districtlevel$lag_murders <- lag.listw(rswm_IPC, IPC_combined_India_districtlevel$murders)
DV <- IPC_combined_India_districtlevel$lag_murders - mean(IPC_combined_India_districtlevel$lag_murders)     
LM_I <- localMI[,1]   
signif <- 0.05       
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4    
quadrant[localMI[,5]>signif] <- 0
```

## Plotting LISA Map

```{r}
IPC.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

tm_shape(IPC.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)
```

```{r}
murders <- qtm(IPC_combined_India_districtlevel, "murders")

IPC.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(IPC.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

tmap_arrange(murders, LISAmap, 
             asp=1, ncol=2)
```

# Hot Spot and Cold Spot Area Analysis

## Deriving Distance Based Weight Matrix

```{r}
longitude <- map_dbl(IPC_combined_India_districtlevel$geometry, ~st_centroid(.x)[[1]])
```

```{r}
latitude <- map_dbl(IPC_combined_India_districtlevel$geometry, ~st_centroid(.x)[[2]])
```

```{r}
coords <- cbind(longitude, latitude)
```

## Determine the Cut-off distance

```{r}
#coords <- coordinates(hunan)
k1_IPC <- knn2nb(knearneigh(coords))
k1dists_IPC <- unlist(nbdists(k1_IPC, coords, longlat = TRUE))
summary(k1dists_IPC)
```

## Computing Fixed Distance Weight Matrix

```{r}
wm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)
wm_d62
```

```{r}
wm62_lw <- nb2listw(wm_d62, style = 'B')
summary(wm62_lw)
```

## Computing Adaptive Distance Weight Matrix

```{r}
knn <- knn2nb(knearneigh(coords, k=6))
knn
```

```{r}
knn_lw <- nb2listw(knn, style = 'B')
summary(knn_lw)
```

## Computing Gi Statistics

```{r}
fips <- order(IPC_combined_India_districtlevel$District)
gi.fixed <- localG(IPC_combined_India_districtlevel$murders, wm62_lw)
gi.fixed
```

```{r}
IPC.gi <- cbind(IPC_combined_India_districtlevel, as.matrix(gi.fixed)) %>%
  rename(gstat_fixed = as.matrix.gi.fixed.)
```

## Mapping Gi values with fixed distance weights

```{r}
murders1 <- qtm(IPC_combined_India_districtlevel, "murders")

Gimap <-tm_shape(IPC.gi) +
  tm_fill(col = "gstat_fixed", 
          style = "pretty",
          palette="-RdBu",
          title = "local Gi") +
  tm_borders(alpha = 0.5)

tmap_arrange(murders1, Gimap, asp=1, ncol=2)
```

## Gi statistics using adaptive distance

```{r}
fips <- order(IPC_combined_India_districtlevel$murders)
gi.adaptive <- localG(IPC_combined_India_districtlevel$murders, knn_lw)
IPC.gi1 <- cbind(IPC_combined_India_districtlevel, as.matrix(gi.adaptive)) %>%
  rename(gstat_adaptive = as.matrix.gi.adaptive.)
```

## Mapping Gi values with adaptive distance weights

```{r}
murders2 <- qtm(IPC_combined_India_districtlevel, "murders")

Gimap <- tm_shape(IPC.gi1) + 
  tm_fill(col = "gstat_adaptive", 
          style = "pretty", 
          palette="-RdBu", 
          title = "local Gi") + 
  tm_borders(alpha = 0.5)

tmap_arrange(murders2, 
             Gimap, 
             asp=1, 
             ncol=2)
```
